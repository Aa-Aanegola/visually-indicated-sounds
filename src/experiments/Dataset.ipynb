{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from pycochleagram.utils import wav_to_array\n",
    "from pycochleagram.cochleagram import human_cochleagram\n",
    "import cv2\n",
    "import numpy as np\n",
    "import scipy\n",
    "import torchvision.transforms as T\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VISDataset(Dataset):\n",
    "    def __init__(self, root, coch_root, dataset_file, window_duration=0.5, datum_len=45, transform=T.Compose([T.Resize((224, 224), antialias=False)]),is_eval=False):\n",
    "        self.root = root\n",
    "        self.coch_root = coch_root\n",
    "        \n",
    "        with open(f'{self.root}/{dataset_file}', 'r') as f:\n",
    "            self.file_list = [file.strip() for file in f.readlines()]\n",
    "        \n",
    "        self.is_eval = is_eval\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.video_fps = 30\n",
    "        self.window_duration = window_duration\n",
    "        self.n_frames = int(window_duration * self.video_fps)\n",
    "        assert datum_len % self.n_frames == 0\n",
    "        self.n_tiles = datum_len // self.n_frames\n",
    "        \n",
    "        self.data = []\n",
    "        \n",
    "        for file in tqdm(self.file_list):\n",
    "            try:    \n",
    "                vid = cv2.VideoCapture(f'{self.root}/{file}_denoised.mp4')\n",
    "                frames = []\n",
    "                while True:\n",
    "                    ret, frame = vid.read()\n",
    "                    if not ret:\n",
    "                        break\n",
    "                    frames.append(frame)\n",
    "                vid.release()\n",
    "                \n",
    "                wav, sample_rate = wav_to_array(f'{self.root}/{file}_denoised.wav')\n",
    "                annotations = pd.read_csv(f'{self.root}/{file}_times.txt', sep=' ', names=['Time', 'Material', 'Action', 'Reaction'])\n",
    "                \n",
    "                cochleagrams = scipy.io.loadmat(f'{self.coch_root}/{file}_sf.mat')['sfs']\n",
    "                \n",
    "                for ind, row in annotations.iterrows():\n",
    "                    datum = {}\n",
    "                    peak_time = row['Time']\n",
    "                    peak_vid = int(peak_time * self.video_fps)\n",
    "                    frames_rgb = np.stack(frames[peak_vid-self.n_frames//2:1+peak_vid+self.n_frames//2]).transpose(0, 3, 1, 2)\n",
    "                    frames_spacetime = np.stack([self.get_spacetime(frames[i-1:i+2]) for i in range(peak_vid-self.n_frames//2, 1+peak_vid+self.n_frames//2)])\n",
    "                    # frames_rgb = np.repeat(frames_rgb, self.n_tiles, axis=0).transpose(0, 3, 1, 2)\n",
    "                    # frames_spacetime = np.repeat(frames_spacetime, self.n_tiles, axis=0)                    \n",
    "                    frames_rgb = self.transform(torch.tensor(frames_rgb))\n",
    "                    frames_spacetime = self.transform(torch.tensor(frames_spacetime))\n",
    "                    \n",
    "                    start_time = peak_time - window_duration/2\n",
    "                    end_time = peak_time + window_duration/2\n",
    "                    start_frame = int(start_time * sample_rate)\n",
    "                    end_frame = int(end_time * sample_rate)\n",
    "                    peak = wav[start_frame:end_frame]\n",
    "                    coch = human_cochleagram(peak, sample_rate, n=40, low_lim=100, hi_lim=10000, sample_factor=1, downsample=90, nonlinearity='power')\n",
    "                            \n",
    "                    datum['frames_rgb'] = frames_rgb\n",
    "                    datum['frames_spacetime'] = frames_spacetime\n",
    "                    # datum['og_cochleagram'] = torch.tensor(cochleagrams[ind])\n",
    "                    datum['cochleagram'] = torch.tensor(coch, dtype=torch.float16).transpose(1, 0)\n",
    "                    datum['material'] = row['Material']\n",
    "                    # datum['action'] = row['Action']\n",
    "                    # datum['reaction'] = row['Reaction']\n",
    "                    # print(datum['frames_rgb'].shape, datum['frames_spacetime'].shape, datum['frames_rgb'].dtype, datum['frames_spacetime'].dtype, datum['cochleagram'].shape, datum['cochleagram'].dtype, datum['action'], datum['material'], datum['reaction'])\n",
    "                    self.data.append(datum)\n",
    "                    \n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)           \n",
    "    \n",
    "    def get_spacetime(self, frames):\n",
    "        return np.stack([cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY) for frame in frames])\n",
    "\n",
    "    def dump(self, root):\n",
    "        for ind, datum in enumerate(self.data):\n",
    "            with open(f'{root}/{ind}.pkl', 'wb') as f:\n",
    "                pickle.dump(datum, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:00<00:03,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/aa_aanegola/miniconda3/envs/cv/lib/python3.9/site-packages/pycochleagram-0.1-py3.9.egg/pycochleagram/cochleagram.py:135: RuntimeWarning: divide by zero encountered in log10\n",
      "  freqs_to_plot = np.log10(freqs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:08<00:14,  4.81s/it][mov,mp4,m4a,3gp,3g2,mj2 @ 0x880b000] moov atom not found\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x85301c0] moov atom not found\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x852fdc0] moov atom not found\n",
      "100%|██████████| 5/5 [00:08<00:00,  1.70s/it]\n"
     ]
    }
   ],
   "source": [
    "ds = VISDataset('../../data/vis-data-256', '../../data/vis-data', 'train_sample.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.dump('../../data/preprocessed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/scratch/aanegola/visually-indicated-sounds/data/preprocessed/'\n",
    "file_list = os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18745/18745 [16:08<00:00, 19.36it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'rock': 1094,\n",
       "             'None': 7163,\n",
       "             'leaf': 1036,\n",
       "             'water': 643,\n",
       "             'wood': 1848,\n",
       "             'plastic-bag': 167,\n",
       "             'ceramic': 120,\n",
       "             'metal': 1717,\n",
       "             'dirt': 1328,\n",
       "             'cloth': 852,\n",
       "             'plastic': 791,\n",
       "             'tile': 108,\n",
       "             'gravel': 131,\n",
       "             'paper': 666,\n",
       "             'drywall': 287,\n",
       "             'glass': 141,\n",
       "             'grass': 476,\n",
       "             'carpet': 177})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = defaultdict(int)\n",
    "for file in tqdm(file_list):\n",
    "    with open(f'{path}{file}', 'rb') as f:\n",
    "         datum = pickle.load(f)\n",
    "         counts[datum['material']] += 1\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9e09ca25bb85b61ef3e58d58363cb7bc503708bb75f3feaddeaa06a257530891"
  },
  "kernelspec": {
   "display_name": "Python 3.9.16 ('cv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
