{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from pycochleagram.utils import wav_to_array\n",
    "from pycochleagram.cochleagram import human_cochleagram\n",
    "import cv2\n",
    "import numpy as np\n",
    "import scipy\n",
    "import torchvision.transforms as T\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VISDataset(Dataset):\n",
    "    def __init__(self, root, coch_root, dataset_file, window_duration=0.5, datum_len=45, transform=T.Compose([T.Resize(256, antialias=False)]),is_eval=False):\n",
    "        self.root = root\n",
    "        self.coch_root = coch_root\n",
    "        \n",
    "        with open(f'{self.root}/{dataset_file}', 'r') as f:\n",
    "            self.file_list = [file.strip() for file in f.readlines()]\n",
    "        \n",
    "        self.is_eval = is_eval\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.video_fps = 30\n",
    "        self.window_duration = window_duration\n",
    "        self.n_frames = int(window_duration * self.video_fps)\n",
    "        assert datum_len % self.n_frames == 0\n",
    "        self.n_tiles = datum_len // self.n_frames\n",
    "        \n",
    "        self.data = []\n",
    "        \n",
    "        for file in tqdm(self.file_list):\n",
    "            try:    \n",
    "                vid = cv2.VideoCapture(f'{self.root}/{file}_denoised.mp4')\n",
    "                frames = []\n",
    "                while True:\n",
    "                    ret, frame = vid.read()\n",
    "                    if not ret:\n",
    "                        break\n",
    "                    frames.append(frame)\n",
    "                vid.release()\n",
    "                \n",
    "                wav, sample_rate = wav_to_array(f'{self.root}/{file}_denoised.wav')\n",
    "                annotations = pd.read_csv(f'{self.root}/{file}_times.txt', sep=' ', names=['Time', 'Material', 'Contact Type', 'Motion Type'])\n",
    "                \n",
    "                cochleagrams = scipy.io.loadmat(f'{self.coch_root}/{file}_sf.mat')['sfs']\n",
    "                \n",
    "                for ind, row in annotations.iterrows():\n",
    "                    datum = {}\n",
    "                    peak_time = row['Time']\n",
    "                    peak_vid = int(peak_time * self.video_fps)\n",
    "                    frames_rgb = np.stack(frames[peak_vid-self.n_frames//2:1+peak_vid+self.n_frames//2])\n",
    "                    frames_spacetime = np.stack([self.get_spacetime(frames[i-1:i+2]) for i in range(peak_vid-self.n_frames//2, 1+peak_vid+self.n_frames//2)])\n",
    "                    frames_rgb = np.repeat(frames_rgb, self.n_tiles, axis=0).transpose(0, 3, 1, 2)\n",
    "                    frames_spacetime = np.repeat(frames_spacetime, self.n_tiles, axis=0)                    \n",
    "                    frames_rgb = self.transform(torch.tensor(frames_rgb))\n",
    "                    frames_spacetime = self.transform(torch.tensor(frames_spacetime))\n",
    "                    \n",
    "                    start_time = peak_time - window_duration/2\n",
    "                    end_time = peak_time + window_duration/2\n",
    "                    start_frame = int(start_time * sample_rate)\n",
    "                    end_frame = int(end_time * sample_rate)\n",
    "                    peak = wav[start_frame:end_frame]\n",
    "                    coch = human_cochleagram(peak, sample_rate, n=40, low_lim=100, hi_lim=10000, sample_factor=1, downsample=90, nonlinearity='power')\n",
    "                            \n",
    "                    datum['frames_rgb'] = frames_rgb\n",
    "                    datum['frames_spacetime'] = frames_spacetime\n",
    "                    datum['og_cochleagram'] = torch.tensor(cochleagrams[ind])\n",
    "                    datum['cochleagram'] = torch.tensor(coch).transpose(1, 0)\n",
    "                    datum['material'] = row['Material']\n",
    "                    self.data.append(datum)\n",
    "                    \n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)           \n",
    "    \n",
    "    def get_spacetime(self, frames):\n",
    "        return np.stack([cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY) for frame in frames])\n",
    "\n",
    "    def dump(self, fname):\n",
    "        with open(fname, 'wb') as f:\n",
    "            pickle.dump(self.data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:00<00:03,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/aa_aanegola/miniconda3/envs/cv/lib/python3.9/site-packages/pycochleagram-0.1-py3.9.egg/pycochleagram/cochleagram.py:135: RuntimeWarning: divide by zero encountered in log10\n",
      "  freqs_to_plot = np.log10(freqs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([45, 42]) torch.Size([45, 42])\n",
      "here\n",
      "torch.Size([45, 42]) torch.Size([45, 42])\n",
      "here\n",
      "torch.Size([45, 42]) torch.Size([45, 42])\n",
      "here\n",
      "torch.Size([45, 42]) torch.Size([45, 42])\n",
      "here\n",
      "torch.Size([45, 42]) torch.Size([45, 42])\n",
      "here\n",
      "torch.Size([45, 42]) torch.Size([45, 42])\n",
      "here\n",
      "torch.Size([45, 42]) torch.Size([45, 42])\n",
      "here\n",
      "torch.Size([45, 42]) torch.Size([45, 42])\n",
      "here\n",
      "torch.Size([45, 42]) torch.Size([45, 42])\n",
      "here\n",
      "torch.Size([45, 42]) torch.Size([45, 42])\n",
      "here\n",
      "torch.Size([45, 42]) torch.Size([45, 42])\n",
      "here\n",
      "torch.Size([45, 42]) torch.Size([45, 42])\n",
      "here\n",
      "torch.Size([45, 42]) torch.Size([45, 42])\n",
      "here\n",
      "torch.Size([45, 42]) torch.Size([45, 42])\n",
      "here\n",
      "torch.Size([45, 42]) torch.Size([45, 42])\n",
      "here\n",
      "torch.Size([45, 42]) torch.Size([45, 42])\n",
      "here\n",
      "torch.Size([45, 42]) torch.Size([45, 42])\n",
      "here\n",
      "torch.Size([45, 42]) torch.Size([45, 42])\n",
      "here\n",
      "torch.Size([45, 42]) torch.Size([45, 42])\n",
      "here\n",
      "torch.Size([45, 42]) torch.Size([45, 42])\n",
      "here\n",
      "torch.Size([45, 42]) torch.Size([45, 42])\n",
      "here\n",
      "torch.Size([45, 42]) torch.Size([45, 42])\n",
      "here\n",
      "torch.Size([45, 42]) torch.Size([45, 42])\n",
      "here\n",
      "torch.Size([45, 42]) torch.Size([45, 42])\n",
      "here\n",
      "torch.Size([45, 42]) torch.Size([45, 42])\n",
      "here\n",
      "torch.Size([45, 42]) torch.Size([45, 42])\n",
      "here\n",
      "torch.Size([45, 42]) torch.Size([45, 42])\n",
      "here\n",
      "torch.Size([45, 42]) torch.Size([45, 42])\n",
      "here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:13<00:23,  7.76s/it][mov,mp4,m4a,3gp,3g2,mj2 @ 0x88c5640] moov atom not found\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x892d340] moov atom not found\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x892cf00] moov atom not found\n",
      "100%|██████████| 5/5 [00:13<00:00,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([45, 42]) torch.Size([45, 42])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ds = VISDataset('../../data/vis-data-256', '../../data/vis-data', 'train_sample.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93\n",
      "dict_keys(['frames_rgb', 'frames_spacetime', 'og_cochleagram', 'material'])\n"
     ]
    }
   ],
   "source": [
    "print(len(ds))\n",
    "print(ds.__getitem__(3).keys())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9e09ca25bb85b61ef3e58d58363cb7bc503708bb75f3feaddeaa06a257530891"
  },
  "kernelspec": {
   "display_name": "Python 3.9.16 ('cv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
