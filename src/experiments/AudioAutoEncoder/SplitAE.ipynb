{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../..')\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.utilities.model_summary import ModelSummary\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "import tickle\n",
    "from tqdm import tqdm\n",
    "from scipy.signal import resample\n",
    "\n",
    "from VISTorchUtils import WaveLoss\n",
    "from VISDataPoint import VISDataPointV3\n",
    "\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioAutoEncoderConv(pl.LightningModule):\n",
    "    def __init__(self, input_size=48000):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, 512, stride=16, padding=8, padding_mode='replicate'),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.RReLU(), \n",
    "            nn.Conv1d(16, 64, 256, stride=4, padding=2, padding_mode='replicate'),\n",
    "            nn.BatchNorm1d(64), \n",
    "            nn.RReLU(), \n",
    "            nn.Conv1d(64, 8, 64, stride=2, padding=1, padding_mode='replicate'),\n",
    "            nn.BatchNorm1d(8))        \n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose1d(8, 64, 64, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.RReLU(), \n",
    "            nn.ConvTranspose1d(64, 16, 256, stride=4, padding=1),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.RReLU(), \n",
    "            nn.ConvTranspose1d(16, 1, 512, stride=16, padding=8))\n",
    "        \n",
    "        # self.loss_fn = nn.MSELoss()\n",
    "        self.loss_fn = WaveLoss()\n",
    "\n",
    "    def forward(self, wav):\n",
    "        emb = self.encoder(wav.unsqueeze(1))\n",
    "        reconstructed = self.decoder(emb).squeeze()\n",
    "        return reconstructed\n",
    "        \n",
    "    def get_encoding(self, wav):\n",
    "        return self.encoder(wav.unsqueeze(1)).flatten(start_dim=1)\n",
    "\n",
    "    def get_reconstructed(self, emb):\n",
    "        return self.decoder(emb.reshape(-1, 8, 310)).squeeze()\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        wav = batch\n",
    "        out = self(wav)\n",
    "        loss = self.loss_fn(out, wav)\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        wav = batch\n",
    "        out = self(wav)\n",
    "        loss = self.loss_fn(out, wav)\n",
    "        self.log('val_loss',  loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), 1e-3)\n",
    "        return optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitAudioAE(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, split_freq:int=5000) -> None:\n",
    "        super().__init__()\n",
    "        self.lfAE = AudioAutoEncoderConv()\n",
    "        self.hfAE = AudioAutoEncoderConv()\n",
    "        self.splitFreq = split_freq\n",
    "\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "    def _splitWaves(self, waves):\n",
    "        freq_data = torch.fft.rfft(waves)\n",
    "       \n",
    "        lf = torch.zeros_like(freq_data)\n",
    "        lf[:,:self.splitFreq] = freq_data[:,:self.splitFreq]\n",
    "\n",
    "        hf = torch.zeros_like(freq_data)\n",
    "        hf[:,self.splitFreq:] = freq_data[:,self.splitFreq:]\n",
    "\n",
    "        lfWave = torch.fft.irfft(lf)\n",
    "        hfWave = torch.fft.irfft(hf)\n",
    "\n",
    "        return lfWave, hfWave\n",
    "    \n",
    "    def _mergeWaves(self, lfWave, hfWave):\n",
    "        \n",
    "        lf = torch.fft.rfft(lfWave)\n",
    "        hf = torch.fft.rfft(hfWave)\n",
    "\n",
    "        # Mask for assistance\n",
    "        lf[:, :self.splitFreq] = 0\n",
    "        hf[:, self.splitFreq:] = 0\n",
    "\n",
    "        freq_data = lf + hf\n",
    "        out = torch.fft.irfft(freq_data)\n",
    "        return out\n",
    "    \n",
    "    def getEmbedding(self, X):\n",
    "        lf, hf = self._splitWaves(X, self.splitFreq)\n",
    "        lfEmb = self.lfAE.get_encoding(lf)\n",
    "        hfEmb = self.hfAE.get_encoding(hf)\n",
    "        return torch.cat([lfEmb, hfEmb], dim=1)\n",
    "    \n",
    "    def reconstructWave(self, emb):\n",
    "        lfEmb, hfEmb = torch.split(emb, emb.shape[1]//2, dim=1)\n",
    "        lfWave = self.lfAE.get_reconstructed(lfEmb)\n",
    "        hfWave = self.hfAE.get_reconstructed(hfEmb)\n",
    "        return self._mergeWaves(lfWave, hfWave)\n",
    "\n",
    "    def forward(self, X):\n",
    "        lf, hf = self._splitWaves(X)\n",
    "        lfOut = self.lfAE(lf)\n",
    "        hfOut = self.hfAE(hf)\n",
    "        out = self._mergeWaves(lfOut, hfOut)\n",
    "        return out\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        wav = batch\n",
    "        out = self(wav)\n",
    "        loss = self.loss_fn(out, wav)\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        wav = batch\n",
    "        out = self(wav)\n",
    "        loss = self.loss_fn(out, wav)\n",
    "        self.log('val_loss',  loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), 1e-3)\n",
    "        return optimizer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, root: str, sr: int=96000):\n",
    "        self.root = root\n",
    "        self.files = glob.glob(os.path.join(self.root, '*.tkl'))\n",
    "        self.sr = sr\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dataPoint: VISDataPointV3 = tickle.load(self.files[idx])\n",
    "        wav = torch.tensor(dataPoint.wav, dtype=torch.float32)\n",
    "        return wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataset = AudioDataset('/scratch/vis_data_v3/train/')\n",
    "valDataset = AudioDataset('/scratch/vis_data_v3/test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "NUM_WORKERS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataLoader = DataLoader(trainDataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "valDataLoader = DataLoader(valDataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SplitAudioAE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "logger = pl.loggers.TensorBoardLogger('tb_logs', name='split_audio_autoencoder')\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    save_top_k=1,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    dirpath=\"model_weights\",\n",
    "    filename=\"split-ae-{epoch:02d}-{val_loss:.2f}\",\n",
    ")\n",
    "trainer = pl.Trainer(accelerator='gpu', devices=1,\n",
    "                     max_epochs=15, logger=logger,\n",
    "                     callbacks=[checkpoint_callback])\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type                 | Params\n",
      "-------------------------------------------------\n",
      "0 | lfAE    | AudioAutoEncoderConv | 606 K \n",
      "1 | hfAE    | AudioAutoEncoderConv | 606 K \n",
      "2 | loss_fn | MSELoss              | 0     \n",
      "-------------------------------------------------\n",
      "1.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 M     Total params\n",
      "4.854     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23b9d4488dd4405eae37ba873facff45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/dhruv.kapur/anaconda3/envs/torch/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4688ff723d7444e3be9941c7835fed78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53ed282cf5bc442994ef2b8d9ea05193",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea438c7460f148cda38fc22f977c1730",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1aa925e56794072be3565d6fec46efb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(model, trainDataLoader, valDataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavs = next(iter(trainDataLoader))\n",
    "wavs_pred = model(wavs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "\n",
    "axs[0].set_title('Ground Truth')\n",
    "axs[0].plot(wavs[0].detach().cpu())\n",
    "axs[1].set_title('Prediction')\n",
    "axs[1].plot(wavs_pred[0].squeeze().detach().cpu())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_data = torch.fft.rfft(wavs[0])\n",
    "freq_data_pred = torch.fft.rfft(wavs_pred[0].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "\n",
    "axs[0].set_title('Ground Truth')\n",
    "axs[0].plot(freq_data[0].detach().cpu())\n",
    "axs[1].set_title('Prediction')\n",
    "axs[1].plot(freq_data_pred[0].squeeze().detach().cpu())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(wavs[0].detach().cpu(), rate=96000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(wavs_pred[0].squeeze().detach().cpu(), rate=96000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
