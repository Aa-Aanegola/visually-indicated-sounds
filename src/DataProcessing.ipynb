{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from pycochleagram.utils import wav_to_array\n",
    "from pycochleagram.cochleagram import human_cochleagram\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple\n",
    "\n",
    "from VISDataPoint import VISDataPoint\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/test.txt', 'r') as f:\n",
    "    file_names = [x.strip() for x in f.readlines()] \n",
    "\n",
    "root = '../../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDatapointsFromFile(file_name, frame_size=(224, 224), window_duration=0.5):\n",
    "    wav_file = os.path.join(root, f'{file_name}_denoised.wav')\n",
    "    video_file = os.path.join(root, f'{file_name}_denoised.mp4')\n",
    "    annotation_file = os.path.join(root, f'{file_name}_times.txt')\n",
    "\n",
    "    annotations = pd.read_csv(annotation_file, sep=' ', names=['Time', 'Material', 'Contact Type', 'Motion Type'])\n",
    "    wav, sample_rate = wav_to_array(wav_file)\n",
    "\n",
    "    cap = cv2.VideoCapture(video_file)\n",
    "    frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frames = []\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        resized_frame = cv2.resize(frame, dsize=frame_size, interpolation=cv2.INTER_CUBIC)\n",
    "        frames.append(resized_frame)\n",
    "    \n",
    "    cap.release()\n",
    "\n",
    "    data_points = []\n",
    "    for row in annotations.iterrows():\n",
    "        peak_time = row[1]['Time']\n",
    "        start_time = peak_time - window_duration/2\n",
    "\n",
    "        start_frame = int(start_time * frame_rate)\n",
    "        end_frame = start_frame + int(frame_rate * window_duration)\n",
    "        window_frames = frames[start_frame-1:end_frame+2]\n",
    "\n",
    "        start_sound = int(start_time * sample_rate)\n",
    "        end_sound = start_sound + int(sample_rate * window_duration)\n",
    "        window_sound = wav[start_sound:end_sound]\n",
    "\n",
    "        coch = human_cochleagram(window_sound, sample_rate, n=40, low_lim=100, hi_lim=10000, sample_factor=1, downsample=90, nonlinearity='power')\n",
    "\n",
    "        data_points.append(VISDataPoint(coch, window_frames, row[1]['Material']))\n",
    "\n",
    "    return data_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 27/244 [02:02<12:34,  3.48s/it][mov,mp4,m4a,3gp,3g2,mj2 @ 0x3884c40] moov atom not found\n",
      " 39%|███▊      | 94/244 [08:05<15:18,  6.12s/it][mov,mp4,m4a,3gp,3g2,mj2 @ 0x4058f80] moov atom not found\n",
      " 96%|█████████▌| 234/244 [21:14<00:32,  3.23s/it][mov,mp4,m4a,3gp,3g2,mj2 @ 0x4058f80] moov atom not found\n",
      "100%|██████████| 244/244 [22:18<00:00,  5.48s/it]\n"
     ]
    }
   ],
   "source": [
    "n_points = 0\n",
    "n_file_fails = 0\n",
    "\n",
    "material_stats = defaultdict(int)\n",
    "\n",
    "for file_name in tqdm(file_names):\n",
    "    try:\n",
    "        data_points = createDatapointsFromFile(file_name)\n",
    "        for data_point in data_points:\n",
    "            material_stats[data_point.material] += 1\n",
    "            with open(f'/scratch/kapur/test/{n_points}.pkl', 'wb') as f:\n",
    "                pickle.dump(data_point, f)\n",
    "            n_points += 1\n",
    "    except:\n",
    "        n_file_fails += 1\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7036"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
